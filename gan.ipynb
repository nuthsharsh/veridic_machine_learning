{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import scipy.ndimage.interpolation\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-c44648db8543>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\snuth\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\snuth\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\snuth\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\snuth\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\snuth\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"mnist\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=10\n",
    "batch_size=128\n",
    "x=tf.placeholder('float',[None,784])\n",
    "y=tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "def maxpool2d(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "def conv2d_transpose(x,W,batch_size,output,out):\n",
    "    return tf.nn.conv2d_transpose(x,W,[batch_size,output,output,out],strides=[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_convolution_gan(x):\n",
    "    weights={\n",
    "        'W_conv1':tf.Variable(tf.random_normal([5,5,1,32])),\n",
    "        'W_conv2':tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "        'W_fc':tf.Variable(tf.random_normal([7*7*64,1024])),\n",
    "        'W_out':tf.Variable(tf.random_normal([1024,n_classes])),\n",
    "        'W_prob':tf.Variable(tf.random_normal([n_classes,1]))\n",
    "    }\n",
    "    biases={\n",
    "        'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "        'b_conv2':tf.Variable(tf.random_normal([64])),\n",
    "        'b_fc':tf.Variable(tf.random_normal([1024])),\n",
    "        'b_out':tf.Variable(tf.random_normal([n_classes])),\n",
    "        'b_prob':tf.Variable(tf.random_normal([1]))\n",
    "    }\n",
    "    x=tf.reshape(x,shape=[-1,28,28,1])\n",
    "    conv1=conv2d(x,weights['W_conv1'])+biases['b_conv1']\n",
    "    conv1 = tf.layers.batch_normalization(conv1)\n",
    "    conv1=tf.nn.leaky_relu(conv1)\n",
    "    conv1=maxpool2d(conv1)\n",
    "    conv2=conv2d(conv1,weights['W_conv2'])+biases['b_conv2']\n",
    "    conv2 =tf.layers.batch_normalization(conv2)\n",
    "    conv2=tf.nn.leaky_relu(conv2)\n",
    "    conv2=maxpool2d(conv2)\n",
    "    fc=tf.reshape(conv2,[-1,7*7*64])\n",
    "    fc=tf.nn.leaky_relu(tf.matmul(fc,weights['W_fc'])+biases['b_fc'])\n",
    "    output=tf.nn.leaky_relu(tf.matmul(fc,weights['W_out'])+biases['b_out'])\n",
    "    prob=tf.nn.sigmoid(tf.matmul(output,weights['W_prob'])+biases['b_prob'])\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#z=np.random.uniform(-1,1,size=[128,100])\n",
    "def generator_convolution_gan(z,batch_size):\n",
    "    weights={\n",
    "        'W_in':tf.Variable(tf.random_normal([100,1024])),\n",
    "        'W_fc':tf.Variable(tf.random_normal([1024,3136])),\n",
    "        'W_conv1':tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "        'W_conv2':tf.Variable(tf.random_normal([5,5,1,32]))\n",
    "    }\n",
    "    biases={\n",
    "        'b_in':tf.Variable(tf.random_normal([1024])),\n",
    "        'b_fc':tf.Variable(tf.random_normal([3136])),\n",
    "        'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "        'b_conv2':tf.Variable(tf.random_normal([1]))\n",
    "        \n",
    "    }\n",
    "    z=tf.reshape(z,shape=[-1,100])\n",
    "    input_g=tf.nn.relu(tf.matmul(z,weights['W_in'])+biases['b_in'])\n",
    "    fc=tf.nn.relu(tf.matmul(input_g,weights['W_fc'])+biases['b_fc'])\n",
    "    fc=tf.reshape(fc,[-1,7,7,64])\n",
    "    conv1=conv2d_transpose(fc,weights['W_conv1'],batch_size,14,32)+biases['b_conv1']\n",
    "    conv1 = tf.layers.batch_normalization(conv1)\n",
    "    conv1=tf.nn.relu(conv1)\n",
    "    #conv1=maxpool2d(conv1)\n",
    "    #conv1=tf.reshape(fc,[-1,14,14,32])\n",
    "    conv2=conv2d_transpose(conv1,weights['W_conv2'],batch_size,28,1)+biases['b_conv2']\n",
    "    conv2 = tf.layers.batch_normalization(conv2)\n",
    "    conv2=tf.nn.tanh(conv2)\n",
    "    #conv2=maxpool2d(conv2)\n",
    "    conv2=tf.reshape(conv2,[-1,784])\n",
    "    return conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed out of 3 loss: 708.9436501264572\n",
      "Epoch 0 completed out of 3 loss_g: 134.38926404714584\n",
      "Epoch 1 completed out of 3 loss: 697.7785792350769\n",
      "Epoch 1 completed out of 3 loss_g: 134.38926404714584\n",
      "Epoch 2 completed out of 3 loss: 697.7785792350769\n",
      "Epoch 2 completed out of 3 loss_g: 134.38926404714584\n"
     ]
    }
   ],
   "source": [
    "y_true=tf.placeholder('float')\n",
    "y_fake=tf.placeholder('float')\n",
    "#samples=tf.placeholder('float')\n",
    "\n",
    "z=tf.placeholder('float',[None,100])\n",
    "def train_neural_network(x):\n",
    "    D_real=discriminator_convolution_gan(x)\n",
    "    G=generator_convolution_gan(z,batch_size)\n",
    "    D_fake=discriminator_convolution_gan(G)\n",
    "    D_real_cost=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real,labels=y_true))\n",
    "    D_fake_cost=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake,labels=y_fake))\n",
    "    G_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake,labels=y_true))\n",
    "    D_loss=D_real_cost+D_fake_cost\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        optimizer_D=tf.train.AdamOptimizer().minimize(D_loss)#Need to change the optimiser\n",
    "        optimizer_G=tf.train.AdamOptimizer().minimize(G_loss)#Need to change the optimiser\n",
    "    \n",
    "    hm_epochs=3\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        samples=np.zeros([128,28,28])\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss=0\n",
    "            epoch_loss_g=0\n",
    "            for i in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y=mnist.train.next_batch(batch_size)\n",
    "                epoch_z=np.random.uniform(-1,1,size=[batch_size,100])\n",
    "                _,c=sess.run([optimizer_D,D_loss],feed_dict={x:epoch_x,y_true:np.ones([batch_size,1]),y_fake:np.zeros([batch_size,1]),z:epoch_z})\n",
    "                _,g,samples_new=sess.run([optimizer_G,G_loss,G],feed_dict={z:epoch_z,y_true:np.ones([batch_size,1])})\n",
    "                samples_new=np.array(samples_new.reshape(batch_size,28,28))\n",
    "                samples=np.append(samples,samples_new,axis=0)\n",
    "                \n",
    "                epoch_loss=epoch_loss+c\n",
    "                epoch_loss_g=epoch_loss_g+g\n",
    "            print('Epoch',epoch,'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "            print('Epoch',epoch,'completed out of',hm_epochs,'loss_g:',epoch_loss_g)\n",
    "        return samples\n",
    "            #print(sess.run(discriminator.hidden_1_layer['weights'])) This command gives the weights\n",
    "samples=train_neural_network(x)\n",
    "\n",
    "\n",
    "\n",
    "#print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_test=np.array(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=tf.placeholder('float',[None,100])\n",
    "#z=np.random.uniform(-1,1,size=[128,100])\n",
    "def generator_convolution_gan(z):\n",
    "    weights={\n",
    "        'W_in':tf.Variable(tf.random_normal([100,1024])),\n",
    "        'W_fc':tf.Variable(tf.random_normal([1024,7*7*64])),\n",
    "        'W_conv1':tf.Variable(tf.random_normal([5,5,64,32])),\n",
    "        'W_conv2':tf.Variable(tf.random_normal([5,5,32,1]))\n",
    "    }\n",
    "    biases={\n",
    "        'b_in':tf.Variable(tf.random_normal([1024])),\n",
    "        'b_fc':tf.Variable(tf.random_normal([64])),\n",
    "        'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "        'b_conv2':tf.Variable(tf.random_normal([1]))\n",
    "        \n",
    "    }\n",
    "    input_g=tf.nn.relu(tf.matmul(z,weights['W_in'])+biases['b_in'])\n",
    "    fc=tf.nn.relu(tf.matmul(input_g,weights['W_fc'])+biases['b_fc'])\n",
    "    fc=tf.reshape(fc,[-1,7,7,64])\n",
    "    conv1=conv2d(fc,weights['W_conv1'])+biases['b_conv1']\n",
    "    conv1 = tf.layers.batch_normalization(conv1)\n",
    "    conv1=tf.nn.relu(conv1)\n",
    "    conv1=maxpool2d(conv1)\n",
    "    \n",
    "    conv2=conv2d(conv1,weights['W_conv2'])+biases['b_conv2']\n",
    "    conv2 = tf.layers.batch_normalization(conv2)\n",
    "    conv2=tf.nn.relu(conv2)\n",
    "    conv2=maxpool2d(conv2)\n",
    "    return conv1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-c6ef069a81f7>:47: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch 0 completed out of 10 loss: 1820493.5403060913\n",
      "Epoch 1 completed out of 10 loss: 416692.27976226807\n",
      "Epoch 2 completed out of 10 loss: 223705.87715387344\n",
      "Epoch 3 completed out of 10 loss: 131737.47967386246\n",
      "Epoch 4 completed out of 10 loss: 81825.28821472079\n",
      "Epoch 5 completed out of 10 loss: 50157.09018534573\n",
      "Epoch 6 completed out of 10 loss: 35186.85095919743\n",
      "Epoch 7 completed out of 10 loss: 25908.77319342326\n",
      "Epoch 8 completed out of 10 loss: 23067.286876805127\n",
      "Epoch 9 completed out of 10 loss: 19883.85368725538\n",
      "Accuracy: 0.9498\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "n_classes = 10\n",
    "batch_size = 100\n",
    "\n",
    "x = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def neural_network_model(data):\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                    'biases':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    # OLD VERSION:\n",
    "    #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\n",
    "    # NEW:\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    hm_epochs = 10\n",
    "    with tf.Session() as sess:\n",
    "        # OLD:\n",
    "        #sess.run(tf.initialize_all_variables())\n",
    "        # NEW:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "\n",
    "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 1)\n",
      "[[[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]]\n"
     ]
    }
   ],
   "source": [
    "z=tf.placeholder('float',[None,100])\n",
    "gg=generator_convolution_gan(z)\n",
    "ggd=discriminator_convolution_gan(gg)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epoch_zz=np.random.uniform(-1,1,size=[128,100])\n",
    "    ggd=sess.run([ggd],feed_dict={z:epoch_zz})\n",
    "ggd=np.array(ggd)\n",
    "print(ggd.shape)\n",
    "print(ggd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.ones([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-e792d68a2d06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\snuth\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \"\"\"\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "a=np.vstack((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-a55686076747>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "a[3,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[3:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.zeros([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164864, 28, 28)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  1.,  1., -1.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164864, 28, 28)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_test=(0.5*samples_test)+0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_test[128:,0:,0:]=255*((0.5*samples_test[128:,0:,0:])+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[255.   0.   0.   0.   0.   0. 255. 255. 255. 255. 255.   0. 255. 255.\n",
      "   255.   0. 255.   0. 255.   0. 255. 255.   0.   0.   0. 255. 255.   0.]\n",
      "  [  0.   0.   0. 255.   0. 255.   0. 255. 255. 255.   0. 255.   0. 255.\n",
      "     0. 255. 255.   0.   0. 255.   0. 255.   0. 255.   0. 255.   0. 255.]\n",
      "  [255.   0. 255. 255.   0.   0.   0. 255. 255.   0.   0.   0.   0. 255.\n",
      "     0. 255.   0.   0.   0.   0.   0. 255.   0.   0.   0. 255. 255.   0.]\n",
      "  [  0.   0.   0. 255. 255. 255.   0.   0.   0. 255.   0. 255. 255. 255.\n",
      "   255. 255. 255. 255.   0. 255. 255. 255. 255.   0. 255. 255. 255.   0.]\n",
      "  [  0. 255. 255. 255.   0. 255.   0. 255. 255. 255. 255. 255. 255. 255.\n",
      "     0. 255. 255. 255. 255.   0.   0. 255. 255. 255. 255.   0.   0.   0.]\n",
      "  [  0. 255.   0. 255. 255. 255. 255. 255.   0. 255.   0.   0.   0.   0.\n",
      "     0.   0.   0.   0.   0. 255.   0.   0.   0. 255. 255. 255.   0. 255.]\n",
      "  [255.   0.   0. 255. 255. 255.   0.   0. 255. 255.   0. 255.   0. 255.\n",
      "     0.   0.   0.   0. 255. 255. 255. 255. 255. 255. 255. 255. 255. 255.]\n",
      "  [255.   0. 255. 255.   0. 255.   0. 255.   0. 255.   0.   0.   0. 255.\n",
      "     0.   0. 255. 255.   0. 255.   0. 255.   0.   0.   0. 255.   0.   0.]\n",
      "  [255. 255. 255. 255.   0. 255. 255.   0.   0. 255. 255.   0. 255. 255.\n",
      "   255. 255.   0. 255. 255. 255. 255. 255. 255. 255.   0. 255. 255.   0.]\n",
      "  [  0.   0.   0. 255.   0.   0.   0. 255.   0.   0.   0. 255.   0.   0.\n",
      "     0. 255.   0.   0.   0. 255.   0.   0.   0. 255.   0.   0. 255. 255.]\n",
      "  [255. 255. 255.   0.   0.   0. 255. 255.   0.   0.   0. 255. 255. 255.\n",
      "   255.   0. 255.   0. 255. 255.   0. 255. 255.   0.   0.   0.   0.   0.]\n",
      "  [255. 255. 255. 255.   0. 255.   0. 255. 255. 255.   0.   0. 255. 255.\n",
      "     0. 255. 255. 255.   0. 255.   0. 255. 255. 255.   0. 255. 255.   0.]\n",
      "  [255. 255. 255.   0. 255. 255. 255. 255.   0. 255.   0. 255. 255. 255.\n",
      "   255. 255.   0. 255. 255. 255. 255. 255. 255.   0. 255. 255. 255.   0.]\n",
      "  [  0.   0.   0. 255.   0.   0.   0. 255.   0.   0.   0. 255.   0. 255.\n",
      "     0. 255.   0.   0.   0. 255. 255.   0.   0.   0. 255.   0.   0. 255.]\n",
      "  [255. 255.   0. 255.   0.   0. 255. 255. 255. 255. 255. 255. 255. 255.\n",
      "     0. 255.   0.   0. 255. 255.   0. 255. 255. 255. 255. 255.   0.   0.]\n",
      "  [255.   0.   0.   0.   0. 255. 255.   0.   0. 255. 255.   0. 255.   0.\n",
      "   255. 255.   0. 255. 255. 255.   0. 255.   0. 255. 255. 255.   0.   0.]\n",
      "  [255. 255. 255. 255. 255. 255.   0.   0.   0.   0. 255. 255.   0. 255.\n",
      "   255. 255. 255. 255. 255.   0. 255. 255. 255. 255. 255. 255. 255.   0.]\n",
      "  [255.   0.   0. 255. 255.   0. 255.   0.   0. 255.   0. 255. 255.   0.\n",
      "   255.   0.   0.   0.   0. 255. 255.   0.   0. 255. 255.   0. 255. 255.]\n",
      "  [255. 255.   0. 255.   0.   0.   0. 255. 255.   0. 255.   0.   0.   0.\n",
      "   255. 255. 255. 255. 255. 255.   0.   0. 255. 255.   0. 255. 255.   0.]\n",
      "  [255.   0. 255. 255.   0. 255.   0.   0.   0. 255.   0.   0.   0. 255.\n",
      "   255.   0.   0. 255. 255.   0.   0. 255.   0. 255.   0. 255.   0.   0.]\n",
      "  [255. 255. 255. 255.   0. 255. 255. 255. 255. 255.   0.   0.   0.   0.\n",
      "   255.   0. 255. 255. 255.   0. 255. 255. 255. 255. 255. 255.   0.   0.]\n",
      "  [  0.   0. 255. 255.   0.   0.   0.   0.   0.   0.   0. 255. 255.   0.\n",
      "     0. 255.   0. 255.   0. 255. 255.   0.   0.   0.   0. 255. 255. 255.]\n",
      "  [255. 255. 255.   0.   0. 255. 255. 255. 255.   0.   0. 255. 255. 255.\n",
      "   255.   0.   0.   0.   0. 255. 255. 255. 255.   0. 255.   0.   0.   0.]\n",
      "  [  0. 255.   0. 255.   0. 255. 255. 255.   0. 255.   0. 255. 255. 255.\n",
      "   255.   0.   0. 255.   0. 255.   0. 255. 255. 255.   0. 255.   0. 255.]\n",
      "  [255. 255. 255.   0. 255. 255. 255. 255.   0. 255. 255.   0. 255. 255.\n",
      "   255. 255. 255. 255. 255.   0. 255.   0. 255. 255.   0. 255. 255.   0.]\n",
      "  [255.   0. 255.   0.   0.   0.   0. 255. 255.   0. 255. 255.   0.   0.\n",
      "     0. 255. 255.   0. 255.   0.   0.   0.   0. 255.   0.   0. 255.   0.]\n",
      "  [255. 255.   0. 255.   0.   0. 255. 255. 255. 255. 255.   0. 255.   0.\n",
      "     0. 255. 255.   0.   0. 255. 255.   0. 255.   0. 255.   0.   0. 255.]\n",
      "  [  0.   0. 255.   0. 255.   0. 255.   0.   0.   0.   0.   0. 255. 255.\n",
      "     0.   0. 255.   0.   0.   0. 255. 255. 255.   0.   0. 255.   0.   0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(samples_test[128:129,0:,0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "im=samples_test[128:,0:,0:].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(im[0:,0,0].shape[0]):\n",
    "    img=im[i,0:,0:]\n",
    "    if i%(859*16)==0:\n",
    "        img=img.reshape([28,28])\n",
    "        img=Image.fromarray(img)\n",
    "        img = img.resize((400, 400))\n",
    "        img.save('testing{}_{}.png'.format(i/(859*16),np.round(i/(859*16*8))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164736"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im[0:,0,0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
